import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
import numpy as np

# Generate synthetic dataset (10,000 rows)
np.random.seed(42)
n = 10000
data = {
    'City': np.random.choice(['New Delhi', 'Mumbai', 'Bangalore', 'Chennai', 'Kolkata'], n),
    'Cuisines': np.random.choice(['Italian', 'Chinese', 'Indian', 'Mexican', 'American'], n),
    'Price range': np.random.randint(1, 5, n),  # 1-4 scale
    'Has Table booking': np.random.choice(['Yes', 'No'], n),
    'Has Online delivery': np.random.choice(['Yes', 'No'], n),
    'Votes': np.random.poisson(500, n),  # Average 500 votes
    'Aggregate rating': np.random.uniform(1.0, 5.0, n)  # Target
}
df = pd.DataFrame(data)

# Separate features and target
X = df.drop(columns=['Aggregate rating'])
y = df['Aggregate rating']

# Identify categorical and numerical columns
categorical_cols = X.select_dtypes(include=['object']).columns
numerical_cols = X.select_dtypes(include=['number']).columns

# Preprocessing pipeline
numerical_transformer = SimpleImputer(strategy='mean')
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_cols),
        ('cat', categorical_transformer, categorical_cols)
    ])

# Apply preprocessing
X_processed = preprocessor.fit_transform(X)

# Split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(X_processed, y, test_size=0.2, random_state=42)

print(f"Training set size: {X_train.shape[0]}")
print(f"Testing set size: {X_test.shape[0]}")
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor

# Train Linear Regression
lr_model = LinearRegression()
lr_model.fit(X_train, y_train)

# Train Decision Tree Regressor
dt_model = DecisionTreeRegressor(random_state=42)
dt_model.fit(X_train, y_train)

print("Models trained successfully.")
from sklearn.metrics import mean_squared_error, r2_score

# Predictions
lr_pred = lr_model.predict(X_test)
dt_pred = dt_model.predict(X_test)

# Evaluate Linear Regression
lr_mse = mean_squared_error(y_test, lr_pred)
lr_r2 = r2_score(y_test, lr_pred)

# Evaluate Decision Tree
dt_mse = mean_squared_error(y_test, dt_pred)
dt_r2 = r2_score(y_test, dt_pred)

print("Linear Regression Performance:")
print(f"  MSE: {lr_mse:.4f}")
print(f"  R²: {lr_r2:.4f}")

print("\nDecision Tree Performance:")
print(f"  MSE: {dt_mse:.4f}")
print(f"  R²: {dt_r2:.4f}")
import matplotlib.pyplot as plt

# For Linear Regression: Get feature names after one-hot encoding
feature_names = preprocessor.named_transformers_['cat']['onehot'].get_feature_names_out(categorical_cols).tolist() + numerical_cols.tolist()

# Coefficients
lr_coeffs = lr_model.coef_
coeff_df = pd.DataFrame({'Feature': feature_names, 'Coefficient': lr_coeffs})
coeff_df = coeff_df.sort_values(by='Coefficient', key=abs, ascending=False)
print("Top Influential Features (Linear Regression):")
print(coeff_df.head(10))

# Plot top coefficients (simulated description)
plt.figure(figsize=(10, 6))
plt.barh(coeff_df['Feature'][:10], coeff_df['Coefficient'][:10])
plt.xlabel('Coefficient Value')
plt.title('Top 10 Feature Coefficients (Linear Regression)')
plt.show()  # In real run, this would display a horizontal bar chart

# For Decision Tree: Feature importances
dt_importances = dt_model.feature_importances_
importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': dt_importances})
importance_df = importance_df.sort_values(by='Importance', ascending=False)
print("\nTop Influential Features (Decision Tree):")
print(importance_df.head(10))

# Plot top importances (simulated description)
plt.figure(figsize=(10, 6))
plt.barh(importance_df['Feature'][:10], importance_df['Importance'][:10])
plt.xlabel('Importance')
plt.title('Top 10 Feature Importances (Decision Tree)')
plt.show()  # In real run, this would display a horizontal bar chart